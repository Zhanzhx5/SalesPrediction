import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
import gc
import os
warnings.filterwarnings('ignore')

# å¯¼å…¥ä¸­å›½èŠ‚å‡æ—¥åº“
import chinese_calendar as cc

# æ•°å€¼ç‰¹å¾ç™½åå•
NUMERICAL_FEATURES = [
    # ä»·æ ¼ç›¸å…³
    'nvipprice',        # VIPä»·æ ¼
    'sgprice',          # åŠç‰Œä»·
    'nsaleprice',       # é”€å”®ä»·æ ¼
    
    # åº—é“ºé¢ç§¯/äººå‘˜
    'nstorearea',       # åº—é“ºé¢ç§¯
    'number_assistant', # åº—å‘˜æ•°é‡
    
    # æ‰€æœ‰åŸºäºå†å²é”€é‡è®¡ç®—å‡ºçš„ç‰¹å¾
    'sales_lag_1',      # å†å²å¹³ç§»ç‰¹å¾
    'sales_lag_3',
    'sales_lag_7',
    'sales_lag_14',
    'sales_lag_28',
    'sales_lag_364',
    'rolling_mean_7',   # æ»‘åŠ¨çª—å£ç‰¹å¾
    'rolling_std_7',
    'rolling_mean_14',
    'rolling_std_14',
    'rolling_mean_30',
    'rolling_max_30',
    'rolling_min_30',
    'last_year_rolling_mean_7',  # å»å¹´åŒæœŸæ»‘åŠ¨ç‰¹å¾
    'last_year_rolling_std_7',
    'YoY_growth_rate_7', # åŒæ¯”å¢é•¿ç‡
    
    # æ–°å¢çš„æ—¶é—´å·®ç‰¹å¾
    'item_age',         # å•†å“ä¸Šå¸‚å¤©æ•°
    'store_age',        # åº—é“ºå¼€ä¸šå¤©æ•°
    
    # æ–°å¢çš„åº“å­˜å’Œé”€é‡é—´éš”ç‰¹å¾
    'zd_kc',            # åœ¨åº—åº“å­˜
    'days_since_last_sale'  # è·ç¦»ä¸Šä¸€æ¬¡æœ‰é”€é‡çš„å¤©æ•°
]

print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†ï¼ˆå…¨éƒ¨åº—é“º+å…¨éƒ¨å•†å“ï¼‰...")
print(f"å¼€å§‹æ—¶é—´: {datetime.now()}")

# 1. è¯»å–åŸºç¡€æ•°æ®
print("ğŸ“– è¯»å–åŸºç¡€æ•°æ®...")
# è¯»å–å…¨éƒ¨å•†å“ä¿¡æ¯
goods_df = pd.read_csv('å•†å“ä¿¡æ¯.csv', dtype=str, encoding='utf-8-sig', keep_default_na=False)
store_df = pd.read_csv('åº—é“ºä¿¡æ¯.csv', dtype=str, encoding='utf-8-sig', keep_default_na=False)

goods_df = goods_df.rename(columns={'sgoodsskc': 'item_id'})
store_df = store_df.rename(columns={'sstoreno': 'store_id'})

print(f"å…¨éƒ¨å•†å“ä¿¡æ¯: {len(goods_df)}è¡Œ, åº—é“ºä¿¡æ¯: {len(store_df)}è¡Œ")

# 2. è®¾ç½®å‚æ•°
start_date = pd.to_datetime('2022-01-01')
end_date = pd.to_datetime('2025-06-30')
all_dates = pd.date_range(start=start_date, end=end_date, freq='D')

# ğŸ”§ ä¿®å¤ï¼šå…ˆè·å–IDåˆ—è¡¨ï¼Œå†è®¾ç½®ç´¢å¼•
# ğŸš€ å¤„ç†å…¨éƒ¨åº—é“ºå’Œå…¨éƒ¨å•†å“
all_stores_full = store_df['store_id'].unique()
all_stores = list(all_stores_full)  # å…¨éƒ¨åº—é“º
all_items = goods_df['item_id'].unique()  # å…¨éƒ¨å•†å“

print(f"ğŸ¯ æ•°æ®èŒƒå›´: å¤„ç†å…¨éƒ¨åº—é“ºï¼Œå…±{len(all_stores_full)}ä¸ª")
print(f"ğŸ¯ å•†å“èŒƒå›´: å…¨éƒ¨å•†å“ {len(all_items)}ä¸ª")
print(f"é€‰ä¸­çš„åº—é“º: {', '.join(all_stores)}")

# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„å¤„ç†å•†å“åº—é“ºç‰¹å¾ï¼Œé¿å…é‡å¤merge
print("âš¡ é¢„å¤„ç†å•†å“åº—é“ºç‰¹å¾...")
# è®¾ç½®å•†å“å’Œåº—é“ºIDä¸ºç´¢å¼•ï¼Œæå‡mergeæ•ˆç‡
goods_df = goods_df.set_index('item_id')
store_df = store_df.set_index('store_id')
print("âœ… å•†å“åº—é“ºç‰¹å¾é¢„å¤„ç†å®Œæˆ")

print(f"æ—¥æœŸ: {len(all_dates)}å¤©, åº—é“º: {len(all_stores)}ä¸ª, å•†å“: {len(all_items)}ä¸ª")
print(f"ğŸ“Š é¢„è®¡æ€»æ•°æ®é‡: {len(all_dates) * len(all_stores) * len(all_items):,} è¡Œ (å®Œæ•´ç‰ˆ)")

# 3. é¢„è®¡ç®—èŠ‚å‡æ—¥ï¼ˆå‘é‡åŒ–ï¼‰
print("ğŸ—“ï¸ é¢„è®¡ç®—èŠ‚å‡æ—¥...")
holiday_array = np.array([cc.is_holiday(date.date()) for date in all_dates], dtype=np.int8)

# 4. åˆ†å—è¯»å–é”€å”®æ•°æ®
print("ğŸ’¾ åˆ†å—è¯»å–é”€å”®æ•°æ®...")
# è®¾ç½®åˆ†å—å¤§å°ï¼ˆæ ¹æ®å†…å­˜æƒ…å†µè°ƒæ•´ï¼‰
CHUNK_SIZE = 1000000  # æ¯æ¬¡è¯»å–100ä¸‡è¡Œ

# é¢„åˆ†é…æ—¶é—´ç‰¹å¾æ•°ç»„
n_dates = len(all_dates)
time_features = np.zeros((n_dates, 7), dtype=np.int16)
date_to_idx = {}
for i, date in enumerate(all_dates):
    date_to_idx[date] = i
    time_features[i] = [
        date.year, date.month, date.day, date.dayofweek,
        date.isocalendar().week, int(date.dayofweek >= 5), holiday_array[i]
    ]

# 6. è¾“å‡ºæ–‡ä»¶è®¾ç½®
output_file = '../model_data.csv'  # è¾“å‡ºåˆ°ä¸Šçº§ç›®å½•

print(f"ğŸ¯ è¾“å‡ºæ–‡ä»¶: {output_file} (å®Œæ•´ç‰ˆ: å…¨éƒ¨åº—é“º+å…¨éƒ¨å•†å“)")

# 7. åˆ†å—å¤„ç†é”€å”®æ•°æ®
print("ğŸ“Š å¼€å§‹åˆ†å—å¤„ç†é”€å”®æ•°æ®...")

# åˆå§‹åŒ–é”€å”®æ•°æ®å­˜å‚¨
all_sales_data = []

# åˆ†å—è¯»å–é”€å”®æ•°æ®
chunk_count = 0
for chunk in pd.read_csv('é”€å”®ä¿¡æ¯_ç²¾ç®€.csv', 
                        dtype={'dtdate': str, 'sstoreno': str, 'sgoodsskc': str, 'fquantity': str, 'åœ¨åº—åº“å­˜': str},
                        encoding='utf-8-sig', 
                        keep_default_na=False,
                        chunksize=CHUNK_SIZE):
    
    chunk_count += 1
    print(f"ğŸ“– å¤„ç†ç¬¬ {chunk_count} å—æ•°æ® ({len(chunk):,} è¡Œ)...")
    
    # æ•°æ®æ¸…ç†ï¼ˆå‘é‡åŒ–ï¼‰
    chunk = chunk.rename(columns={'sstoreno': 'store_id', 'sgoodsskc': 'item_id'})
    valid_mask = chunk['dtdate'].str.match(r'^\d{4}-\d{2}-\d{2}$', na=False)
    chunk = chunk[valid_mask]

    chunk['dtdate'] = pd.to_datetime(chunk['dtdate'], errors='coerce')
    chunk['sales'] = pd.to_numeric(chunk['fquantity'], errors='coerce').fillna(0)
    chunk['zd_kc'] = pd.to_numeric(chunk['åœ¨åº—åº“å­˜'], errors='coerce').fillna(0)

    # å°†è´Ÿæ•°çš„é”€é‡å’Œåœ¨åº—åº“å­˜æ”¹ä¸º0
    chunk['sales'] = chunk['sales'].clip(lower=0)
    chunk['zd_kc'] = chunk['zd_kc'].clip(lower=0)

    chunk = chunk[(chunk['dtdate'] >= start_date) & (chunk['dtdate'] <= end_date)]
    chunk = chunk.dropna(subset=['dtdate'])

    # èšåˆé”€å”®æ•°æ®ï¼ˆé”€é‡æ±‚å’Œï¼Œåœ¨åº—åº“å­˜å–æœ€åå€¼ï¼‰
    chunk = chunk.groupby(['dtdate', 'store_id', 'item_id']).agg({
        'sales': 'sum',
        'zd_kc': 'last'  # åœ¨åº—åº“å­˜å–æœ€åå€¼ï¼Œå› ä¸ºåº“å­˜æ˜¯çŠ¶æ€å€¼
    }).reset_index()

    # ğŸ¯ åªä¿ç•™å…¨éƒ¨åº—é“ºçš„é”€å”®æ•°æ®
    chunk = chunk[chunk['store_id'].isin(all_stores) & chunk['item_id'].isin(all_items)]
    
    if len(chunk) > 0:
        all_sales_data.append(chunk)
    
    # æ˜¾ç¤ºè¿›åº¦
    if chunk_count % 5 == 0:
        print(f"   å·²å¤„ç† {chunk_count} å—ï¼Œå½“å‰ç´¯è®¡æœ‰æ•ˆæ•°æ®: {sum(len(df) for df in all_sales_data):,} è¡Œ")

# åˆå¹¶æ‰€æœ‰é”€å”®æ•°æ®
if all_sales_data:
    sales_df = pd.concat(all_sales_data, ignore_index=True)
    del all_sales_data  # é‡Šæ”¾å†…å­˜
    gc.collect()
    
    # å†æ¬¡èšåˆï¼ˆå¯èƒ½æœ‰é‡å¤çš„åº—é“º-å•†å“-æ—¥æœŸç»„åˆï¼‰
    sales_df = sales_df.groupby(['dtdate', 'store_id', 'item_id']).agg({
        'sales': 'sum',
        'zd_kc': 'last'
    }).reset_index()
    
    print(f"âœ… é”€å”®æ•°æ®å¤„ç†å®Œæˆï¼Œæ€»è®¡: {len(sales_df):,} è¡Œ")
else:
    sales_df = pd.DataFrame(columns=['dtdate', 'store_id', 'item_id', 'sales', 'zd_kc'])
    print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„é”€å”®æ•°æ®")

# 8. åˆ†å—åˆ›å»ºåŸºç¡€æ¡†æ¶å’Œå¤„ç†æ•°æ®
print("ğŸ“Š åˆ†å—åˆ›å»ºåŸºç¡€æ¡†æ¶...")

# è®¡ç®—æ¯ä¸ªå•†å“çš„å¤„ç†æ‰¹æ¬¡å¤§å°ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
ITEMS_PER_CHUNK = 100  # æ¯æ¬¡å¤„ç†100ä¸ªå•†å“
total_chunks = (len(all_items) + ITEMS_PER_CHUNK - 1) // ITEMS_PER_CHUNK

print(f"ğŸ“Š å°†åˆ† {total_chunks} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {ITEMS_PER_CHUNK} ä¸ªå•†å“")

# åˆå§‹åŒ–è¾“å‡ºæ–‡ä»¶
first_chunk = True

for chunk_idx in range(total_chunks):
    start_idx = chunk_idx * ITEMS_PER_CHUNK
    end_idx = min((chunk_idx + 1) * ITEMS_PER_CHUNK, len(all_items))
    current_items = all_items[start_idx:end_idx]
    
    print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {chunk_idx + 1}/{total_chunks}: å•†å“ {start_idx + 1}-{end_idx} (å…±{len(current_items)}ä¸ª)")
    
    # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„åŸºç¡€æ¡†æ¶
    n_combinations = len(all_dates) * len(all_stores) * len(current_items)
    
    chunk_df = pd.DataFrame({
        'dtdate': np.repeat(all_dates, len(all_stores) * len(current_items)),
        'store_id': np.tile(np.repeat(all_stores, len(current_items)), len(all_dates)),
        'item_id': np.tile(current_items, len(all_dates) * len(all_stores)),
        'sales': 0.0,
        'zd_kc': 0.0
    })
    
    print(f"   åŸºç¡€æ¡†æ¶: {n_combinations:,} è¡Œ")
    
    # å¡«å……é”€å”®æ•°æ®
    print("   ğŸ’° å¡«å……é”€å”®æ•°æ®...")
    current_sales = sales_df[sales_df['item_id'].isin(current_items)]
    
    if len(current_sales) > 0:
        # ä½¿ç”¨mergeæ“ä½œ
        chunk_df = chunk_df.merge(
            current_sales[['dtdate', 'store_id', 'item_id', 'sales', 'zd_kc']], 
            on=['dtdate', 'store_id', 'item_id'], 
            how='left',
            suffixes=('', '_actual')
        )
        # ç”¨å®é™…é”€é‡å’Œåœ¨åº—åº“å­˜å¡«å……ï¼Œä¿æŒ0ä¸ºé»˜è®¤å€¼
        chunk_df['sales'] = chunk_df['sales_actual'].fillna(0.0)
        chunk_df['zd_kc'] = chunk_df['zd_kc_actual'].fillna(0.0)
        chunk_df = chunk_df.drop(['sales_actual', 'zd_kc_actual'], axis=1)
    
    print(f"   é”€å”®æ•°æ®å¡«å……å®Œæˆï¼Œéé›¶é”€é‡: {(chunk_df['sales'] > 0).sum()} è¡Œ")
    
    # ğŸ”¥ ä¿®å¤åœ¨åº—åº“å­˜é€»è¾‘ï¼šä½¿ç”¨å‰å‘å¡«å……æ²¿ç”¨ä¸Šä¸€ä¸ªæœ‰è®°å½•çš„åœ¨åº—åº“å­˜
    print("   ğŸ“¦ ä¿®å¤åœ¨åº—åº“å­˜é€»è¾‘ï¼šå‰å‘å¡«å……...")
    chunk_df = chunk_df.sort_values(['store_id', 'item_id', 'dtdate']).reset_index(drop=True)
    
    # æŒ‰å•†å“-åº—é“ºç»„åˆè¿›è¡Œå‰å‘å¡«å……
    chunk_df['zd_kc'] = chunk_df.groupby(['store_id', 'item_id'])['zd_kc'].fillna(method='ffill').fillna(0.0)
    
    print(f"   åœ¨åº—åº“å­˜å‰å‘å¡«å……å®Œæˆï¼Œéé›¶åº“å­˜: {(chunk_df['zd_kc'] > 0).sum()} è¡Œ")
    
    # æ·»åŠ æ—¶é—´ç‰¹å¾ï¼ˆå‘é‡åŒ–ï¼‰
    print("   ğŸ“… æ·»åŠ æ—¶é—´ç‰¹å¾...")
    date_indices = np.array([date_to_idx[date] for date in chunk_df['dtdate']])
    chunk_df['year'] = time_features[date_indices, 0]
    chunk_df['month'] = time_features[date_indices, 1] 
    chunk_df['day'] = time_features[date_indices, 2]
    chunk_df['day_of_week'] = time_features[date_indices, 3]
    chunk_df['week_of_year'] = time_features[date_indices, 4]
    chunk_df['is_weekend'] = time_features[date_indices, 5]
    chunk_df['is_holiday'] = time_features[date_indices, 6]
    
    # å†å²ç‰¹å¾è®¡ç®—ï¼ˆé«˜é€Ÿå‘é‡åŒ–ç‰ˆæœ¬ï¼‰
    print("   ğŸ“ˆ è®¡ç®—å†å²ç‰¹å¾ï¼ˆé«˜é€Ÿç‰ˆï¼‰...")
    chunk_df = chunk_df.sort_values(['store_id', 'item_id', 'dtdate']).reset_index(drop=True)
    
    # ä½¿ç”¨pandaså‘é‡åŒ–æ“ä½œè®¡ç®—æ‰€æœ‰ç‰¹å¾
    def calculate_features_vectorized(group):
        # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
        group = group.sort_values('dtdate')
        # è½¬numpyæ•°ç»„å¹¶è½¬æ¢ä¸ºfloat32ï¼Œæå‡æ€§èƒ½
        sales = group['sales'].values.astype('float32')
        n = len(sales)
        
        # é¢„åˆ†é…ç»“æœæ•°ç»„
        result_arrays = {}
        
        # 1. æ»åç‰¹å¾ï¼ˆç›´æ¥ç´¢å¼•ï¼Œæ— éœ€è¾¹ç•Œåˆ¤æ–­ï¼‰
        lag_periods = [1, 3, 7, 14, 28, 364]
        for lag in lag_periods:
            result_arrays[f'sales_lag_{lag}'] = np.concatenate([np.zeros(lag), sales[:-lag]])
        
        # 2. æ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ï¼ˆåŸºäºå‰Nå¤©ï¼Œä¸å«å½“å¤©ï¼‰
        # 7å¤©æ»šåŠ¨ï¼ˆåŸºäºå‰7å¤©ï¼Œä¸å«å½“å¤©ï¼‰
        result_arrays['rolling_mean_7'] = pd.Series(sales).shift(1).rolling(window=7, min_periods=1).mean().values
        result_arrays['rolling_std_7'] = pd.Series(sales).shift(1).rolling(window=7, min_periods=1).std(ddof=0).values
        
        # 14å¤©æ»šåŠ¨ï¼ˆåŸºäºå‰14å¤©ï¼Œä¸å«å½“å¤©ï¼‰
        result_arrays['rolling_mean_14'] = pd.Series(sales).shift(1).rolling(window=14, min_periods=1).mean().values
        result_arrays['rolling_std_14'] = pd.Series(sales).shift(1).rolling(window=14, min_periods=1).std(ddof=0).values
        
        # 30å¤©æ»šåŠ¨ï¼ˆåŸºäºå‰30å¤©ï¼Œä¸å«å½“å¤©ï¼‰
        result_arrays['rolling_mean_30'] = pd.Series(sales).shift(1).rolling(window=30, min_periods=1).mean().values
        result_arrays['rolling_max_30'] = pd.Series(sales).shift(1).rolling(window=30, min_periods=1).max().values
        result_arrays['rolling_min_30'] = pd.Series(sales).shift(1).rolling(window=30, min_periods=1).min().values
        
        # 3. åŒæ¯”ç‰¹å¾
        # å»å¹´åŒæœŸ7å¤©æ»šåŠ¨å‡å€¼ï¼ˆåŸºäºå»å¹´åŒæœŸå‰7å¤©ï¼‰
        result_arrays['last_year_rolling_mean_7'] = pd.Series(sales).shift(365).rolling(window=7, min_periods=1).mean().values
        result_arrays['last_year_rolling_std_7'] = pd.Series(sales).shift(365).rolling(window=7, min_periods=1).std(ddof=0).values
        
        # åŒæ¯”å¢é•¿ç‡
        current_mean = result_arrays['rolling_mean_7']
        last_year_mean = result_arrays['last_year_rolling_mean_7']
        result_arrays['YoY_growth_rate_7'] = np.where(
            last_year_mean > 0, 
            (current_mean - last_year_mean) / last_year_mean, 
            0
        )
        
        # 4. è®¡ç®—è·ç¦»ä¸Šä¸€æ¬¡æœ‰é”€é‡çš„å¤©æ•°ï¼ˆå®Œå…¨å‘é‡åŒ–ç‰ˆæœ¬ï¼Œä¸¥æ ¼é¿å…æ•°æ®æ³„éœ²ï¼‰
        # ä½¿ç”¨pandasçš„å‘é‡åŒ–æ“ä½œï¼Œå®Œå…¨é¿å…Pythonå¾ªç¯
        sales_series = pd.Series(sales)
        n = len(sales_series)
        
        # åˆ›å»ºé”€é‡å¤§äº0çš„æ©ç 
        sales_gt_zero = sales_series > 0
        
        # å®Œå…¨å‘é‡åŒ–çš„å®ç°ï¼Œç¡®ä¿åªçœ‹å†å²æ•°æ®
        # å…³é”®æ€è·¯ï¼šä½¿ç”¨shift(1)å°†é”€é‡ä¿¡æ¯å‘åç§»åŠ¨ä¸€ä½ï¼Œç¡®ä¿å½“å¤©é”€é‡ä¸è¢«ä½¿ç”¨
        
        # 1. å°†é”€é‡æ©ç å‘åç§»åŠ¨ä¸€ä½ï¼ˆè¿™æ ·æ¯ä¸ªä½ç½®åªèƒ½çœ‹åˆ°å‰ä¸€å¤©åŠä¹‹å‰çš„é”€é‡ä¿¡æ¯ï¼‰
        historical_sales_gt_zero = sales_gt_zero.shift(1, fill_value=False)
        
        # 2. ä¸ºæ¯ä¸ªä½ç½®åˆ›å»ºç´¢å¼•
        position_index = pd.Series(range(n))
        
        # 3. åˆ›å»ºä¸€ä¸ªSeriesæ¥è®°å½•æ¯ä¸ªæœ‰é”€é‡ä½ç½®çš„ç´¢å¼•
        # åªæœ‰å†å²æ•°æ®ä¸­æœ‰é”€é‡çš„ä½ç½®æ‰ä¼šè¢«è®°å½•
        last_sale_positions = pd.Series(index=position_index.index, dtype='float64')
        
        # 4. ä½¿ç”¨å‘é‡åŒ–æ“ä½œï¼šå¯¹äºæ¯ä¸ªæœ‰å†å²é”€é‡çš„ä½ç½®ï¼Œè®°å½•å…¶ç´¢å¼•
        # ç„¶åä½¿ç”¨forward fillæ¥ä¼ æ’­åˆ°åç»­ä½ç½®
        last_sale_positions[historical_sales_gt_zero] = position_index[historical_sales_gt_zero]
        last_sale_positions = last_sale_positions.fillna(method='ffill')
        
        # 5. è®¡ç®—è·ç¦»ä¸Šä¸€æ¬¡æœ‰é”€é‡çš„å¤©æ•°
        days_since_last_sale = position_index - last_sale_positions
        
        # 6. å¤„ç†æ²¡æœ‰å†å²é”€é‡çš„æƒ…å†µï¼ˆè®¾ä¸º10000.0ï¼‰
        days_since_last_sale = days_since_last_sale.fillna(10000.0)
        
        # 7. ç¡®ä¿ç¬¬ä¸€ä¸ªä½ç½®æ˜¯10000.0ï¼ˆæ²¡æœ‰å†å²æ•°æ®ï¼‰
        if n > 0:
            days_since_last_sale.iloc[0] = 10000.0
        
        # 8. è½¬æ¢ä¸ºfloat32ç±»å‹ï¼ˆä¿æŒä¸å…¶ä»–æ•°å€¼ç‰¹å¾ä¸€è‡´ï¼‰
        result_arrays['days_since_last_sale'] = days_since_last_sale.astype('float32').values
        
        # 9. è¶…é«˜æ•ˆæ‰¹é‡èµ‹å€¼åˆ°DataFrameï¼ˆç›´æ¥æ“ä½œåº•å±‚æ•°æ®ï¼‰
        # ä½¿ç”¨pandasçš„concatè¿›è¡Œæ‰¹é‡æ·»åŠ åˆ—ï¼ˆæœ€é«˜æ•ˆï¼‰
        new_columns_df = pd.DataFrame(result_arrays, index=group.index)
        group = pd.concat([group, new_columns_df], axis=1)
        
        return group
    
    # æ‰¹é‡è®¡ç®—æ‰€æœ‰å†å²ç‰¹å¾ï¼ˆæ¯”å¾ªç¯å¿«100å€ä»¥ä¸Šï¼‰
    print("   âš¡ æ‰§è¡Œå‘é‡åŒ–ç‰¹å¾è®¡ç®—...")
    chunk_df = chunk_df.groupby(['store_id', 'item_id'], group_keys=False).apply(calculate_features_vectorized)
    
    print(f"   âœ… å†å²ç‰¹å¾è®¡ç®—å®Œæˆï¼ˆå‘é‡åŒ–ç‰ˆæœ¬ï¼‰")
    
    # æ·»åŠ å•†å“å’Œåº—é“ºç‰¹å¾ï¼ˆé«˜æ•ˆç‰ˆï¼‰
    print("   ğŸª æ·»åŠ å•†å“åº—é“ºç‰¹å¾ï¼ˆé«˜æ•ˆç‰ˆï¼‰...")
    # ä½¿ç”¨ç´¢å¼•mergeï¼Œé¿å…é‡å¤ç­›é€‰
    chunk_df = chunk_df.merge(goods_df, left_on='item_id', right_index=True, how='left')
    chunk_df = chunk_df.merge(store_df, left_on='store_id', right_index=True, how='left')
    
    # è®¡ç®—item_ageå’Œstore_ageç‰¹å¾
    print("   ğŸ“… è®¡ç®—æ—¶é—´å·®ç‰¹å¾...")
    if 'market_date' in chunk_df.columns:
        chunk_df['market_date'] = pd.to_datetime(chunk_df['market_date'], errors='coerce')
        chunk_df['item_age'] = (chunk_df['dtdate'] - chunk_df['market_date']).dt.days
        chunk_df['item_age'] = chunk_df['item_age'].fillna(0)
        chunk_df = chunk_df.drop('market_date', axis=1)
        print("   âœ… å·²è®¡ç®—item_ageç‰¹å¾")
    
    if 'dopentime' in chunk_df.columns:
        chunk_df['dopentime'] = pd.to_datetime(chunk_df['dopentime'], errors='coerce')
        chunk_df['store_age'] = (chunk_df['dtdate'] - chunk_df['dopentime']).dt.days
        chunk_df['store_age'] = chunk_df['store_age'].fillna(0)
        chunk_df = chunk_df.drop('dopentime', axis=1)
        print("   âœ… å·²è®¡ç®—store_ageç‰¹å¾")
    
    # æŒ‰ç…§ç™½åå•è§„åˆ™è®¾ç½®æ•°æ®ç±»å‹ï¼ˆé«˜æ•ˆç‰ˆï¼‰
    print("   ğŸ”§ æŒ‰ç…§ç™½åå•è§„åˆ™è®¾ç½®æ•°æ®ç±»å‹...")
    
    # è¯†åˆ«æ‰€æœ‰ç‰¹å¾åˆ—
    exclude_cols = ['dtdate', 'sales']
    feature_cols = [col for col in chunk_df.columns if col not in exclude_cols]
    
    # æŒ‰ç…§ç™½åå•åˆ†ç¦»æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾
    numeric_cols = []
    categorical_cols = []
    
    for col in feature_cols:
        if col in NUMERICAL_FEATURES:
            numeric_cols.append(col)
        else:
            categorical_cols.append(col)
    
    # ğŸ”¥ ä¿®å¤ï¼šå…ˆå¤„ç†æ•°å€¼ç‰¹å¾ä¸­çš„æ— æ•ˆå€¼ï¼Œå†è®¾ç½®æ•°æ®ç±»å‹
    print("   ğŸ”§ å¤„ç†æ•°å€¼ç‰¹å¾ä¸­çš„æ— æ•ˆå€¼...")
    for col in numeric_cols:
        if col in chunk_df.columns:
            # å¤„ç†ç©ºå­—ç¬¦ä¸²ã€NaNç­‰æ— æ•ˆå€¼
            chunk_df[col] = pd.to_numeric(chunk_df[col], errors='coerce').fillna(0.0)
    
    # æ„å»ºæ•°æ®ç±»å‹å­—å…¸
    dtype_dict = {}
    
    # è®¾ç½®åˆ†ç±»ç‰¹å¾ä¸ºcategoryç±»å‹
    for col in categorical_cols:
        if col in chunk_df.columns:
            dtype_dict[col] = 'category'
    
    # è®¾ç½®æ•°å€¼ç‰¹å¾çš„æ•°æ®ç±»å‹
    for col in numeric_cols:
        if col in chunk_df.columns:
            dtype_dict[col] = 'float32'  # æ‰€æœ‰æ•°å€¼ç‰¹å¾ç»Ÿä¸€ä½¿ç”¨float32
    
    # è®¾ç½®åŸºç¡€å­—æ®µçš„æ•°æ®ç±»å‹
    dtype_dict['store_id'] = 'category'
    dtype_dict['item_id'] = 'category'
    dtype_dict['sales'] = 'float32'
    dtype_dict['zd_kc'] = 'float32'
    
    # ä¸€æ¬¡æ€§è½¬æ¢æ‰€æœ‰åˆ—çš„æ•°æ®ç±»å‹
    chunk_df = chunk_df.astype(dtype_dict)
    print("   âœ… æ•°æ®ç±»å‹è®¾ç½®å®Œæˆ")
    
    # è¿‡æ»¤ä¿å­˜æ•°æ®ï¼ˆæå¤´å»å°¾ï¼šåªä¿ç•™æ¯ä¸ªåº—é“º-å•†å“ç»„åˆä»ç¬¬ä¸€æ¬¡æœ‰é”€é‡åˆ°æœ€åä¸€æ¬¡æœ‰é”€é‡çš„æœŸé—´ï¼‰
    print("   ğŸ’¾ è¿‡æ»¤å¹¶ä¿å­˜æ•°æ®ï¼ˆæå¤´å»å°¾å¤„ç†ï¼‰...")
    save_start_date = pd.to_datetime('2023-01-10')
    
    # é¦–å…ˆè¿‡æ»¤å‡º2023-01-10åŠä¹‹åçš„æ•°æ®
    chunk_df_after_2023 = chunk_df[chunk_df['dtdate'] >= save_start_date].copy()
    
    # å¯¹æ¯ä¸ªåº—é“º-å•†å“ç»„åˆè¿›è¡Œæå¤´å»å°¾å¤„ç†ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    print("   ğŸ”§ æ‰§è¡Œæå¤´å»å°¾å¤„ç†ï¼ˆå‘é‡åŒ–ä¼˜åŒ–ï¼‰...")
    
    # ä½¿ç”¨pandasçš„å‘é‡åŒ–æ“ä½œï¼Œé¿å…Pythonå¾ªç¯
    # 1. è®¡ç®—æ¯ä¸ªç»„åˆçš„ç¬¬ä¸€æ¬¡å’Œæœ€åä¸€æ¬¡æœ‰é”€é‡çš„æ—¥æœŸ
    sales_info = chunk_df_after_2023[chunk_df_after_2023['sales'] > 0].groupby(['store_id', 'item_id'])['dtdate'].agg(['min', 'max']).reset_index()
    sales_info.columns = ['store_id', 'item_id', 'first_sale_date', 'last_sale_date']
    
    # 2. æ ¹æ®è§„åˆ™è°ƒæ•´å°¾éƒ¨ä¿ç•™é€»è¾‘ï¼š
    #    å¦‚æœæœ€åä¸€æ¬¡æœ‰é”€é‡æ˜¯ 2025-05-01 åŠä»¥åï¼ˆå« 5 æœˆï¼‰ï¼Œåˆ™æŠŠå°¾ä¿ç•™åˆ° 2025-06-30
    may_first_2025 = pd.to_datetime('2025-05-01')
    jun_30_2025 = pd.to_datetime('2025-06-30')
    sales_info['tail_end_date'] = sales_info['last_sale_date'].where(
        sales_info['last_sale_date'] < may_first_2025,
        jun_30_2025
    )
    
    # 3. å°†è°ƒæ•´åçš„è¾¹ç•Œåˆå¹¶å›åŸæ•°æ®
    chunk_df_with_bounds = chunk_df_after_2023.merge(
        sales_info[['store_id', 'item_id', 'first_sale_date', 'tail_end_date']],
        on=['store_id', 'item_id'],
        how='inner'
    )
    
    # 4. ä½¿ç”¨å‘é‡åŒ–æ“ä½œè¿‡æ»¤æ•°æ®ï¼ˆæå¤´å»å°¾ï¼Œå¸¦å°¾éƒ¨å»¶é•¿ï¼‰
    chunk_df_filtered = chunk_df_with_bounds[
        (chunk_df_with_bounds['dtdate'] >= chunk_df_with_bounds['first_sale_date']) &
        (chunk_df_with_bounds['dtdate'] <= chunk_df_with_bounds['tail_end_date'])
    ].copy()
    
    # 4. åˆ é™¤è¾…åŠ©åˆ—å¹¶æ’åº
    chunk_df_filtered = chunk_df_filtered.drop(['first_sale_date', 'tail_end_date'], axis=1)
    chunk_df_filtered = chunk_df_filtered.sort_values(['store_id', 'item_id', 'dtdate']).reset_index(drop=True)
    
    print(f"   âœ… å‘é‡åŒ–æå¤´å»å°¾å¤„ç†å®Œæˆï¼Œä¿ç•™äº† {len(sales_info)} ä¸ªæœ‰é”€é‡çš„åº—é“º-å•†å“ç»„åˆ")
    
    # ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    if first_chunk:
        chunk_df_filtered.to_csv(output_file, index=False, encoding='utf-8-sig')
        first_chunk = False
    else:
        chunk_df_filtered.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
    
    print(f"   ğŸ’¾ å·²ä¿å­˜æ‰¹æ¬¡ {chunk_idx + 1} æ•°æ®: {len(chunk_df_filtered):,} è¡Œ")
    
    # æ¸…ç†å†…å­˜
    del chunk_df, chunk_df_after_2023, chunk_df_with_bounds, chunk_df_filtered
    gc.collect()
    
    # æ˜¾ç¤ºè¿›åº¦
    progress = (chunk_idx + 1) / total_chunks * 100
    print(f"   ğŸ“Š æ€»ä½“è¿›åº¦: {progress:.1f}%")

# 14. è¾“å‡ºç»Ÿè®¡
print("=" * 60)
print("ğŸ‰ å®Œæ•´æ•°æ®å¤„ç†å®Œæˆ! (å…¨éƒ¨åº—é“º+å…¨éƒ¨å•†å“)")
print(f"ç»“æŸæ—¶é—´: {datetime.now()}")

# è®¡ç®—æœ€ç»ˆæ–‡ä»¶å¤§å°
if os.path.exists(output_file):
    output_size = os.path.getsize(output_file) / (1024**3)
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
    print(f"è¾“å‡ºæ–‡ä»¶å¤§å°: {output_size:.2f} GB")
else:
    print("âš ï¸ è­¦å‘Šï¼šè¾“å‡ºæ–‡ä»¶æœªç”Ÿæˆ")

print("=" * 60)
print("æ•°æ®è¯´æ˜:")
print(f"- ğŸ¯ æ•°æ®èŒƒå›´: å…¨éƒ¨åº—é“ºæ•°æ®ï¼ˆå…±{len(all_stores)}å®¶ï¼‰")
print("- ğŸ¯ å•†å“èŒƒå›´: å…¨éƒ¨å•†å“")
print("- ğŸ¯ æ—¶é—´èŒƒå›´: æå¤´å»å°¾å¤„ç†ï¼ˆæ¯ä¸ªåº—é“º-å•†å“ç»„åˆä»ç¬¬ä¸€æ¬¡æœ‰é”€é‡åˆ°æœ€åä¸€æ¬¡æœ‰é”€é‡ï¼‰")
print("- æ ‡ç­¾å­—æ®µ (y): sales (å½“æ—¥é”€é‡)")
print("- ç‰¹å¾å­—æ®µ (X): é™¤saleså¤–çš„æ‰€æœ‰å­—æ®µï¼ŒåŒ…æ‹¬zd_kc (åœ¨åº—åº“å­˜)")  
print("- âœ… å†å²ç‰¹å¾è®¡ç®—é‡‡ç”¨pandaså‘é‡åŒ–æ“ä½œï¼ˆé€Ÿåº¦æå‡100å€+ï¼‰")
print("- âœ… å•†å“åº—é“ºç‰¹å¾mergeä¼˜åŒ–ï¼ˆä½¿ç”¨ç´¢å¼•æå‡æ•ˆç‡ï¼‰")
print("- âœ… æŒ‰ç…§ç™½åå•è§„åˆ™è®¾ç½®æ•°æ®ç±»å‹ï¼ˆæ•°å€¼ç‰¹å¾ç»Ÿä¸€float32ï¼Œåˆ†ç±»ç‰¹å¾categoryï¼‰")
print("- âœ… ä¿®å¤æ•°å€¼ç‰¹å¾æ— æ•ˆå€¼å¤„ç†ï¼šå¤„ç†ç©ºå­—ç¬¦ä¸²ç­‰æ— æ•ˆå€¼ï¼Œé¿å…ç±»å‹è½¬æ¢é”™è¯¯")
print("- âœ… ä¿®å¤åœ¨åº—åº“å­˜é€»è¾‘ï¼šä½¿ç”¨å‰å‘å¡«å……æ²¿ç”¨ä¸Šä¸€ä¸ªæœ‰è®°å½•çš„åœ¨åº—åº“å­˜å€¼")
print("- âœ… ä¿®å¤æ ‡å‡†å·®è®¡ç®—ä¸€è‡´æ€§ï¼ˆä½¿ç”¨ddof=0ï¼‰")
print("- âœ… æ‰€æœ‰å†å²ç‰¹å¾ä¸¥æ ¼åŸºäºå†å²æ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²")
print("- âœ… æ»šåŠ¨ç‰¹å¾åŸºäºå‰Nå¤©ï¼ˆä¸å«å½“å¤©ï¼‰")
print("- âœ… å†å²ç‰¹å¾å……åˆ†ï¼ˆ2022å¹´æ•°æ®ç”¨äºè®¡ç®—ï¼Œæå¤´å»å°¾åä¿å­˜ï¼‰")
print("- âœ… æå¤´å»å°¾å¤„ç†ï¼šæ¯ä¸ªåº—é“º-å•†å“ç»„åˆåªä¿ç•™ä»ç¬¬ä¸€æ¬¡æœ‰é”€é‡åˆ°æœ€åä¸€æ¬¡æœ‰é”€é‡çš„æœŸé—´")
print("- âœ… é›¶é”€é‡è®°å½•ï¼šä¿ç•™æœŸé—´å†…çš„é›¶é”€é‡è®°å½•ï¼Œç¡®ä¿æ—¶é—´åºåˆ—è¿ç»­æ€§")
print("- âœ… æ— é”€é‡ç»„åˆï¼šå®Œå…¨è·³è¿‡ä»æœªæœ‰é”€é‡çš„åº—é“º-å•†å“ç»„åˆ")
print("- âœ… æ–°å¢ç‰¹å¾: days_since_last_sale (è·ç¦»ä¸Šä¸€æ¬¡æœ‰é”€é‡çš„å¤©æ•°)")
print("- âœ… æ–°å¢ç‰¹å¾: item_age (å•†å“ä¸Šå¸‚å¤©æ•°), store_age (åº—é“ºå¼€ä¸šå¤©æ•°)")
print("- âœ… åˆ†å—å¤„ç†ï¼šé¿å…å†…å­˜æº¢å‡ºï¼Œæ”¯æŒå¤§æ•°æ®é›†å¤„ç†")
print("- âœ… åˆ†å—è¯»å–ï¼šé”€å”®æ•°æ®åˆ†å—è¯»å–ï¼Œæ¯å—100ä¸‡è¡Œ")
print("- âœ… åˆ†å—ç”Ÿæˆï¼šæŒ‰å•†å“åˆ†å—ç”Ÿæˆï¼Œæ¯æ‰¹100ä¸ªå•†å“")
print(f"- ğŸ’¡ å®Œæ•´ç‰ˆæœ¬ï¼Œå¤„ç†å…¨éƒ¨æ•°æ®ï¼Œé€‚åˆæœ€ç»ˆæ¨¡å‹è®­ç»ƒ")

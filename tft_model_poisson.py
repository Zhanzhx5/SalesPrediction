#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é”€é‡é¢„æµ‹é¡¹ç›® - TFTæ¨¡å‹

ä¸¥æ ¼æŒ‰ç…§æŒ‡å¯¼å®ç°çš„Temporal Fusion Transformeræ¨¡å‹
åŸºäºPyTorch Forecastingæ¡†æ¶
"""

import pandas as pd
import numpy as np
import torch
import pytorch_lightning as pl
from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, NaNLabelEncoder
from pytorch_forecasting.metrics import NegativeBinomialDistributionLoss, PoissonLoss
from pytorch_forecasting.data.encoders import TorchNormalizer
from baseline_model import calculate_wape, calculate_mae, calculate_rmse
from pytorch_lightning.strategies import DDPSpawnStrategy
import warnings
import random
warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°
def set_random_seed(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿ç»“æœå¯é‡ç°"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # å¦‚æœä½¿ç”¨å¤šGPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    pl.seed_everything(seed, workers=True)
    print(f"âœ… éšæœºç§å­å·²è®¾ç½®ä¸º: {seed}")

# ä¿®å¤numpyå…¼å®¹æ€§é—®é¢˜
# ä¸ºæ—§ç‰ˆæœ¬çš„pytorch-forecastingæä¾›np.floatå…¼å®¹æ€§
if not hasattr(np, 'float'):
    np.float = float

class TFTModel:
    """Temporal Fusion Transformeræ¨¡å‹"""
    
    def __init__(self, 
                 prediction_length=30,
                 encoder_length=90,
                 learning_rate=0.0002,
                 hidden_size=64,
                 attention_head_size=8,
                 dropout=0.2,
                 hidden_continuous_size=32,
                 batch_size=1024,
                 max_epochs=30,
                 patience=5,
                 random_seed=42,
                 optuna_pruning_callback=None):
        """
        åˆå§‹åŒ–TFTæ¨¡å‹
        
        Args:
            prediction_length: é¢„æµ‹é•¿åº¦ï¼ˆå¤©æ•°ï¼‰
            encoder_length: ç¼–ç å™¨é•¿åº¦ï¼ˆå›çœ‹å¤©æ•°ï¼‰
            learning_rate: å­¦ä¹ ç‡
            hidden_size: éšè—å±‚å¤§å°
            attention_head_size: æ³¨æ„åŠ›å¤´æ•°é‡
            dropout: Dropoutæ¯”ä¾‹
            hidden_continuous_size: è¿ç»­ç‰¹å¾éšè—å±‚å¤§å°
            batch_size: æ‰¹æ¬¡å¤§å°
            max_epochs: æœ€å¤§è®­ç»ƒè½®æ•°
            patience: æ—©åœè€å¿ƒå€¼
        """
        self.prediction_length = prediction_length
        self.encoder_length = encoder_length
        self.learning_rate = learning_rate
        self.hidden_size = hidden_size
        self.attention_head_size = attention_head_size
        self.dropout = dropout
        self.hidden_continuous_size = hidden_continuous_size
        self.batch_size = batch_size
        self.max_epochs = max_epochs
        self.patience = patience
        self.random_seed = random_seed
        self.optuna_pruning_callback = optuna_pruning_callback
        
        # è®¾ç½®åŸºç¡€éšæœºç§å­ï¼ˆä¸è§¦å‘å¤šè¿›ç¨‹ï¼‰
        random.seed(self.random_seed)
        np.random.seed(self.random_seed)
        torch.manual_seed(self.random_seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(self.random_seed)
            torch.cuda.manual_seed_all(self.random_seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        
        # æ¨¡å‹å’Œæ•°æ®é›†
        self.model = None
        self.training_dataset = None
        self.validation_dataset = None
        self.trainer = None
        
        print("ğŸš€ TFTæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"   - é¢„æµ‹é•¿åº¦: {prediction_length}å¤©")
        print(f"   - ç¼–ç å™¨é•¿åº¦: {encoder_length}å¤©")
        print(f"   - æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   - æœ€å¤§è®­ç»ƒè½®æ•°: {max_epochs}")
        print(f"   - éšæœºç§å­: {random_seed}")
    
    def load_and_preprocess_data(self, file_path):
        """
        æ­¥éª¤1: æ•°æ®åŠ è½½ä¸åŸºç¡€é¢„å¤„ç†
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            é¢„å¤„ç†åçš„æ•°æ®æ¡†
        """
        print("ğŸ“Š æ­¥éª¤1: æ•°æ®åŠ è½½ä¸åŸºç¡€é¢„å¤„ç†")
        
        # 1. åŠ è½½æ‚¨å·²ç»å¤„ç†å¥½çš„CSVæ–‡ä»¶
        print(f"   åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
        df = pd.read_csv(file_path)
        
        # 2. è½¬æ¢æ—¥æœŸæ ¼å¼ (éå¸¸é‡è¦)
        df['dtdate'] = pd.to_datetime(df['dtdate'])
        
        # 3. åˆ›å»ºæ•´æ•°æ—¶é—´ç´¢å¼• `time_idx` (åº“çš„ç¡¬æ€§è¦æ±‚)
        # å®ƒå°†æ—¥æœŸè½¬æ¢ä¸ºä»0å¼€å§‹çš„è¿ç»­æ•´æ•°ï¼Œè¿™æ˜¯TFTå·¥ä½œçš„æ ¸å¿ƒã€‚
        df['time_idx'] = (df['dtdate'] - df['dtdate'].min()).dt.days
        
        # 4. ç»Ÿä¸€è½¬æ¢æ‰€æœ‰åˆ†ç±»ç‰¹å¾çš„æ•°æ®ç±»å‹ (æ¨èæ­¥éª¤)
        # è¿™ä¸€æ­¥å¯ä»¥é˜²æ­¢åç»­å‡ºç°ç±»å‹æ¨æ–­é”™è¯¯ï¼Œå¹¶èƒ½èŠ‚çœå†…å­˜
        all_categorical_feature_names = [
            'store_id', 'item_id', 'year', 'month', 'day', 'day_of_week', 'week_of_year',
            'is_weekend', 'is_holiday', 'skc_sgspztdesc', 'shbd', 'sgoodtype', 'sfabric',
            'sfabricdesc', 'sdeptname', 'sstyle', 'supcolorno', 'sserie', 'susedof',
            'sseason', 'scolordesc', 'scomponet4', 'syear', 'scategorydesc', 'sstoredesc',
            'sdevelopment', 'qydz', 'ywqy', 'sprovince', 'scity', 'saddress', 'szone',
            'schanneltype', 'sstoresize', 'scitylevelid', 'sscmsalelevel', 'sscmarealevel',
            'sscmtotallevel', 'is_bus', 'sleveltype', 'stimage', 'ssalelevel',
            # æ–°æ·»åŠ çš„10ä¸ªå•†å“å±æ€§åˆ—
            'sbartype', 'scollar', 'scottoncupmaterial', 'secseries', 'smoldcupprocess',
            'spattern', 'sshoulderpro', 'sshoulderwidth', 'ssteelwheel', 'swaistband'
        ]
        
        for col in all_categorical_feature_names:
            if col in df.columns:
                df[col] = df[col].astype(str).astype("category")
        
        # 5. ç¡®ä¿æ•°å€¼ç‰¹å¾ä½¿ç”¨å…¼å®¹çš„æ•°æ®ç±»å‹ (è§£å†³numpyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜)
        numerical_features = [
            'sales', 'zd_kc', 'nsaleprice', 'nstorearea', 'number_assistant', 
            'nvipprice', 'sgprice', 'item_age', 'store_age',
            'sales_lag_1', 'sales_lag_3', 'sales_lag_7', 'sales_lag_14', 
            'sales_lag_28', 'sales_lag_364', 'rolling_mean_7', 'rolling_std_7',
            'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_max_30',
            'rolling_min_30', 'last_year_rolling_mean_7', 'last_year_rolling_std_7',
            'YoY_growth_rate_7', 'days_since_last_sale'
        ]
        
        for col in numerical_features:
            if col in df.columns:
                # ä½¿ç”¨float32è€Œä¸æ˜¯np.float32ï¼Œç¡®ä¿å…¼å®¹æ€§
                df[col] = df[col].astype('float32')
        
        # 6. ç¡®ä¿time_idxæ˜¯æ•´æ•°ç±»å‹ (TFTåº“çš„ç¡¬æ€§è¦æ±‚)
        df['time_idx'] = df['time_idx'].astype('int32')
        
        print("âœ… æ•°æ®åŠ è½½ä¸åŸºç¡€é¢„å¤„ç†å®Œæˆ")
        return df
    
    def define_feature_groups(self):
        """
        æ­¥éª¤2: ç‰¹å¾æœ€ç»ˆåˆ†ç»„å®šä¹‰
        
        Returns:
            ç‰¹å¾åˆ†ç»„å­—å…¸
        """
        print("ğŸ”§ æ­¥éª¤2: ç‰¹å¾åˆ†ç»„å®šä¹‰")
        
        # --- é™æ€ç‰¹å¾ (Static Features) ---
        # å¯¹äºä¸€ä¸ªå•†å“-åº—é“ºç»„åˆï¼Œè¿™äº›ç‰¹å¾æ˜¯å›ºå®šä¸å˜çš„ã€‚æ¨¡å‹ä¼šå­¦ä¹ å®ƒä»¬å¯¹æ•´ä¸ªåºåˆ—çš„é•¿æœŸå½±å“ã€‚
        static_categoricals_list = [
            'store_id', 'item_id', 'skc_sgspztdesc', 'shbd', 'sgoodtype', 'sfabric',
            'sfabricdesc', 'sdeptname', 'sstyle', 'supcolorno', 'sserie', 'susedof',
            'sseason', 'scolordesc', 'scomponet4', 'syear', 'scategorydesc', 'sstoredesc',
            'sdevelopment', 'qydz', 'ywqy', 'sprovince', 'scity', 'saddress', 'szone',
            'schanneltype', 'sstoresize', 'scitylevelid', 'sscmsalelevel', 'sscmarealevel',
            'sscmtotallevel', 'is_bus', 'sleveltype', 'stimage', 'ssalelevel',
            # æ·»åŠ ç¼ºå¤±çš„10ä¸ªå•†å“å±æ€§åˆ—
            'sbartype', 'scollar', 'scottoncupmaterial', 'secseries', 'smoldcupprocess',
            'spattern', 'sshoulderpro', 'sshoulderwidth', 'ssteelwheel', 'swaistband'
        ]
        
        # æ ¹æ®æ‚¨çš„æ¾„æ¸…ï¼ŒVIPä»·å’ŒåŠç‰Œä»·æ˜¯é™æ€çš„
        static_reals_list = [
            'nstorearea', 'number_assistant', 'nvipprice', 'sgprice'
        ]

        # --- åŠ¨æ€ä¸”æœªæ¥å·²çŸ¥çš„ç‰¹å¾ (Time-Varying Known Features) ---
        # å¯¹äºæœªæ¥çš„æ¯ä¸€å¤©ï¼Œæˆ‘ä»¬éƒ½èƒ½æå‰çŸ¥é“è¿™äº›ç‰¹å¾çš„å€¼ã€‚æ¨¡å‹ä¼šåˆ©ç”¨å®ƒä»¬æ¥åš"è®¡åˆ’"ã€‚
        time_varying_known_categoricals_list = [
            'year', 'month', 'day', 'day_of_week', 'week_of_year', 'is_weekend', 'is_holiday'
        ]
        time_varying_known_reals_list = [
            'time_idx', 'item_age', 'store_age' 
        ]

        # --- åŠ¨æ€ä¸”æœªæ¥æœªçŸ¥çš„ç‰¹å¾ (Time-Varying Unknown Features) ---
        # è¿™äº›æ˜¯éœ€è¦é¢„æµ‹æˆ–åªèƒ½è§‚æµ‹åˆ°çš„å†å²æ•°æ®ã€‚åœ¨é¢„æµ‹æœªæ¥æ—¶ï¼Œæ¨¡å‹æ— æ³•è·å–è¿™äº›ç‰¹å¾çš„æœªæ¥å€¼ã€‚
        # æˆ‘ä»¬ä¿ç•™æ‰€æœ‰lagå’Œrollingç‰¹å¾ï¼Œå®ƒä»¬èƒ½ä¸ºTFTæä¾›å¼ºå¤§çš„ä¿¡å·"å¿«æ·æ–¹å¼"ï¼Œå¼¥è¡¥å›çœ‹çª—å£çš„è§†é‡å±€é™ã€‚
        time_varying_unknown_reals_list = [
            'sales', 'zd_kc', 'nsaleprice', 'sales_lag_1', 'sales_lag_3', 'sales_lag_7',
            'sales_lag_14', 'sales_lag_28', 'sales_lag_364', 'rolling_mean_7',
            'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30',
            'rolling_max_30', 'rolling_min_30', 'last_year_rolling_mean_7',
            'last_year_rolling_std_7', 'YoY_growth_rate_7', 'days_since_last_sale'
        ]
        
        feature_groups = {
            'static_categoricals': static_categoricals_list,
            'static_reals': static_reals_list,
            'time_varying_known_categoricals': time_varying_known_categoricals_list,
            'time_varying_known_reals': time_varying_known_reals_list,
            'time_varying_unknown_reals': time_varying_unknown_reals_list
        }
        
        print("âœ… ç‰¹å¾åˆ†ç»„å®šä¹‰å®Œæˆ")
        return feature_groups
    
    def create_timeseries_dataset(self, df, feature_groups):
        """
        æ­¥éª¤3: åˆ›å»º TimeSeriesDataSet
        
        Args:
            df: é¢„å¤„ç†åçš„æ•°æ®æ¡†
            feature_groups: ç‰¹å¾åˆ†ç»„å­—å…¸
            
        Returns:
            è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†
        """
        print("ğŸ”§ æ­¥éª¤3: åˆ›å»ºTimeSeriesDataSet")
        
        # æ ¹æ®æ‚¨çš„æ•°æ®åˆ’åˆ†ï¼Œè®­ç»ƒé›†åˆ°2025å¹´4æœˆ30æ—¥
        training_cutoff_date = pd.to_datetime("2025-04-30")
        training_cutoff_idx = df[df['dtdate'] == training_cutoff_date]['time_idx'].iloc[0]
        
        # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„ç‰¹å¾
        for group_name, features in feature_groups.items():
            feature_groups[group_name] = [f for f in features if f in df.columns]

        # --- åˆ›å»ºè®­ç»ƒæ•°æ®é›† ---
        self.training_dataset = TimeSeriesDataSet(
            df[lambda x: x.time_idx <= training_cutoff_idx],
            time_idx="time_idx",
            target="sales",
            group_ids=["store_id", "item_id"],
            max_encoder_length=self.encoder_length,
            max_prediction_length=self.prediction_length,
            min_encoder_length=0,                   # <-- å…³é”®ä¿®æ­£ï¼šå…è®¸æœ€çŸ­0å¤©çš„å†å²
            static_categoricals=feature_groups['static_categoricals'],
            static_reals=feature_groups['static_reals'],
            time_varying_known_categoricals=feature_groups['time_varying_known_categoricals'],
            time_varying_known_reals=feature_groups['time_varying_known_reals'],
            time_varying_unknown_reals=feature_groups['time_varying_unknown_reals'],

            # ç›®æ ‡ä¸åšä»»ä½•å½’ä¸€åŒ–/æ ‡å‡†åŒ–ï¼šidentityï¼ˆæ— éœ€é€†å˜æ¢ï¼‰
            target_normalizer=TorchNormalizer(method="identity", center=False),
            # ä¿ç•™æ‚¨å¿…éœ€çš„åˆ†ç±»ç¼–ç å™¨ï¼Œä»¥å¤„ç†æœªè§è¿‡çš„ç±»åˆ«
            categorical_encoders={
                col: NaNLabelEncoder(add_nan=True) for col in (
                    feature_groups['static_categoricals'] + feature_groups['time_varying_known_categoricals']
                )
            },

            allow_missing_timesteps=True,
            add_relative_time_idx=True
        )
        
        # åˆ›å»ºéªŒè¯æ•°æ®é›† (åªé¢„æµ‹5æœˆä»½ï¼Œç”¨äºè®­ç»ƒæ—¶çš„éªŒè¯)
        # æ‰¾åˆ°éªŒè¯é›†é¢„æµ‹æ‰€éœ€å†å²æ•°æ®çš„æˆªæ­¢ç‚¹
        val_encoder_cutoff_date = pd.to_datetime("2025-04-30")
        val_encoder_cutoff_idx = df[df['dtdate'] == val_encoder_cutoff_date]['time_idx'].iloc[0]
        
        # é™åˆ¶éªŒè¯é›†åªåŒ…å«5æœˆä»½çš„é¢„æµ‹çª—å£
        val_end_date = pd.to_datetime("2025-05-31")
        val_end_idx = df[df['dtdate'] == val_end_date]['time_idx'].iloc[0]
        
        self.validation_dataset = TimeSeriesDataSet.from_dataset(
            self.training_dataset, 
            df[df['time_idx'] <= val_end_idx], # åªä¼ å…¥åˆ°5æœˆ31æ—¥çš„æ•°æ®
            predict=True, 
            stop_randomization=True,
            min_prediction_idx=val_encoder_cutoff_idx + 1,  # ä»5æœˆ1æ—¥å¼€å§‹é¢„æµ‹
            min_encoder_length=0,  # æ˜¾å¼è®¾ç½®min_encoder_length=0
            add_relative_time_idx=True
        )
        
        print(f"âœ… TimeSeriesDataSetåˆ›å»ºå®Œæˆ")
        print(f"   - è®­ç»ƒé›†æ ·æœ¬æ•°: {len(self.training_dataset)}")
        print(f"   - éªŒè¯é›†æ ·æœ¬æ•°: {len(self.validation_dataset)}")
        
        return self.training_dataset, self.validation_dataset
    
    def create_model(self):
        """
        æ­¥éª¤4: æ¨¡å‹é…ç½®ä¸å®ä¾‹åŒ–
        """
        print("ğŸ”§ æ­¥éª¤4: åˆ›å»ºTFTæ¨¡å‹")
        
        self.model = TemporalFusionTransformer.from_dataset(
            self.training_dataset,
            learning_rate=self.learning_rate,
            hidden_size=self.hidden_size,
            attention_head_size=self.attention_head_size,
            dropout=self.dropout,
            hidden_continuous_size=self.hidden_continuous_size,
            output_size=1,  # Poisson åªéœ€è¦ 1 ä¸ªå‚æ•°
            loss=PoissonLoss(),
            log_interval=10,
            reduce_on_plateau_patience=4,
        )
        
        print("âœ… TFTæ¨¡å‹åˆ›å»ºå®Œæˆ")
    
    def train_model(self):
        """
        æ­¥éª¤5: æ¨¡å‹è®­ç»ƒ
        """
        print("ğŸ”§ æ­¥éª¤5: è®­ç»ƒTFTæ¨¡å‹")
        
        # åœ¨è®­ç»ƒå¼€å§‹æ—¶è®¾ç½®PyTorch Lightningçš„éšæœºç§å­ï¼ˆä¼šè§¦å‘å¤šè¿›ç¨‹ï¼‰
        pl.seed_everything(self.random_seed, workers=True)

        # Optional: åˆ©ç”¨ Tensor Coresï¼ŒåŠ é€Ÿ matmul
        try:
            torch.set_float32_matmul_precision('medium')  # or 'high'
        except Exception:
            pass
        
        # åˆ›å»ºDataLoader
        train_dataloader = self.training_dataset.to_dataloader(
            train=True,
            batch_size=self.batch_size,
            num_workers=4,
            persistent_workers=True,
            pin_memory=True,
        )
        val_dataloader = self.validation_dataset.to_dataloader(
            train=False,
            batch_size=self.batch_size * 2,
            num_workers=4,
            persistent_workers=True,
            pin_memory=True,
        )
        
        # æ—©åœå›è°ƒ
        early_stop_callback = pl.callbacks.EarlyStopping(
            monitor="val_loss", min_delta=1e-4, patience=self.patience, verbose=False, mode="min"
        )
        lr_logger = pl.callbacks.LearningRateMonitor()
        
        # æ¨¡å‹æ£€æŸ¥ç‚¹å›è°ƒ - ä½¿ç”¨PyTorch Lightningçš„versionæœºåˆ¶
        checkpoint_callback = pl.callbacks.ModelCheckpoint(
            monitor="val_loss",
            dirpath=None,  # è®©PyTorch Lightningè‡ªåŠ¨ç®¡ç†è·¯å¾„åˆ°versionç›®å½•
            filename="tft_model-{epoch:02d}-{val_loss:.4f}",
            save_top_k=1,  # æ¯ä¸ªversionåªä¿ç•™æœ€å¥½çš„ä¸€ä¸ªæ¨¡å‹
            mode="min",
            save_last=False  # ä¸ä¿å­˜æœ€åä¸€ä¸ªepoch
        )
        
        # 1. æ˜¾å¼åˆ›å»ºä¸€ä¸ªTensorBoard Logger
        from pytorch_lightning.loggers import TensorBoardLogger
        logger = TensorBoardLogger("lightning_logs", name="sales_forecasting_tft")
        
        # 2. æ£€æŸ¥GPUå¹¶è‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
        if torch.cuda.is_available():
            accelerator_config = 'gpu'
            gpu_count = torch.cuda.device_count()
            if gpu_count > 1:
                # å¤šGPUï¼šä½¿ç”¨ DDPSpawnStrategy å¹¶å…³é—­ find_unused_parameters ä»¥é¿å…æ€§èƒ½å¼€é”€è­¦å‘Š
                devices_config = "auto"  # ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
                strategy_config = DDPSpawnStrategy(find_unused_parameters=False)
                print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPUï¼Œå¯ç”¨å¤šGPUè®­ç»ƒ")
            else:
                devices_config = [0]
                strategy_config = None
                print(f"ğŸ–¥ï¸ ä½¿ç”¨å•GPUè®­ç»ƒ")
        else:
            accelerator_config = 'cpu'
            devices_config = 'auto'
            strategy_config = None
            print("âš ï¸ æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼Œå°†ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒã€‚")

        # 3. åˆ›å»ºè®­ç»ƒå™¨ (ä½¿ç”¨å…¼å®¹æœ€æ–°ç‰ˆçš„å‚æ•°)
        callbacks_list = [lr_logger, early_stop_callback, checkpoint_callback]
        if getattr(self, 'optuna_pruning_callback', None) is not None:
            callbacks_list.append(self.optuna_pruning_callback)

        self.trainer = pl.Trainer(
            max_epochs=self.max_epochs,
            accelerator=accelerator_config,
            devices=devices_config,
            strategy=strategy_config,
            gradient_clip_algorithm="norm",
            gradient_clip_val=0.1,
            callbacks=callbacks_list,
            enable_progress_bar=True,
            logger=logger,
        )
        
        # è®­ç»ƒæ¨¡å‹
        self.trainer.fit(
            self.model,
            train_dataloaders=train_dataloader,
            val_dataloaders=val_dataloader,
        )
        
        print("âœ… TFTæ¨¡å‹è®­ç»ƒå®Œæˆ")
    
    def predict_and_evaluate(self, df):
        """
        æ­¥éª¤6: é¢„æµ‹ã€åå¤„ç†ä¸è¯„ä¼° (ä¿®æ­£ç‰ˆ)
        
        Args:
            df: åŒ…å«æ‰€æœ‰æ•°æ®çš„å®Œæ•´é¢„å¤„ç†åDataFrame
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        print("ğŸ”® æ­¥éª¤6: é¢„æµ‹ä¸è¯„ä¼°")

        # 1. åŠ è½½æœ€ä½³æ¨¡å‹
        if self.trainer is not None and hasattr(self.trainer, 'checkpoint_callback') and self.trainer.checkpoint_callback.best_model_path:
            # å¦‚æœå·²ç»è®­ç»ƒè¿‡ï¼Œä½¿ç”¨è®­ç»ƒå™¨ä¸­çš„æœ€ä½³æ¨¡å‹
            best_model_path = self.trainer.checkpoint_callback.best_model_path
            print(f"   åŠ è½½å·²è®­ç»ƒæ¨¡å‹: {best_model_path}")
        else:
            # å¦‚æœåªæ˜¯è¯„ä¼°ï¼Œä»æœ€æ–°çš„versionç›®å½•åŠ è½½æœ€ä½³æ¨¡å‹
            import os
            import glob
            
            lightning_logs_dir = "lightning_logs/sales_forecasting_tft"
            if os.path.exists(lightning_logs_dir):
                # æŸ¥æ‰¾æ‰€æœ‰ç‰ˆæœ¬ç›®å½•
                version_dirs = glob.glob(os.path.join(lightning_logs_dir, "version_*"))
                if version_dirs:
                    # æŒ‰ç‰ˆæœ¬å·æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
                    version_dirs.sort(key=lambda x: int(x.split('version_')[-1]), reverse=True)
                    latest_version_dir = version_dirs[0]
                    checkpoint_dir = os.path.join(latest_version_dir, "checkpoints")
                    
                    if os.path.exists(checkpoint_dir):
                        checkpoint_files = glob.glob(os.path.join(checkpoint_dir, "*.ckpt"))
                        if checkpoint_files:
                            # é€‰æ‹©æœ€æ–°çš„checkpointæ–‡ä»¶
                            checkpoint_files.sort(key=os.path.getmtime, reverse=True)
                            best_model_path = checkpoint_files[0]
                            print(f"   åŠ è½½æœ€æ–°versionçš„æœ€ä½³æ¨¡å‹: {best_model_path}")
                        else:
                            raise FileNotFoundError("æœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒæ­¥éª¤")
                    else:
                        raise FileNotFoundError("æœªæ‰¾åˆ°versionç›®å½•ä¸­çš„checkpointsæ–‡ä»¶å¤¹ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒæ­¥éª¤")
                else:
                    raise FileNotFoundError("æœªæ‰¾åˆ°æ¨¡å‹ç‰ˆæœ¬ç›®å½•ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒæ­¥éª¤")
            else:
                raise FileNotFoundError("æœªæ‰¾åˆ°lightning_logsç›®å½•ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒæ­¥éª¤")
        
        best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)
        
        # ä¸ºé¿å…åœ¨å¤§å‹æ•°æ®è¯„ä¼°æ—¶å‡ºç°è®¾å¤‡ä¸ä¸€è‡´é—®é¢˜ï¼Œç»Ÿä¸€åœ¨CPUä¸Šè¿›è¡Œæ¨ç†
        best_tft.eval()
        best_tft.cpu()
        print("   æ¨¡å‹å·²åŠ è½½åˆ° CPU ç”¨äºæ¨ç†")

        # 2. éªŒè¯æ•°æ®é›†å­˜åœ¨
        if self.validation_dataset is None:
            raise ValueError("validation_datasetä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œcreate_timeseries_datasetæ–¹æ³•")
        
        # 3. éªŒè¯dfå‚æ•°
        if df is None:
            raise ValueError("å¿…é¡»ä¼ å…¥åŒ…å«çœŸå®å€¼çš„å®Œæ•´DataFrameä»¥è¿›è¡Œè¯„ä¼°")
        
        required_columns = ['time_idx', 'store_id', 'item_id', 'sales']
        missing_columns = [col for col in required_columns if col not in df.columns]
        if missing_columns:
            raise ValueError(f"DataFrameç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")

        results = {}

        # 4. é¢„æµ‹éªŒè¯é›† (5æœˆä»½)
        print("   é¢„æµ‹éªŒè¯é›† (5æœˆä»½)...")
        val_dataloader = self.validation_dataset.to_dataloader(
            train=False, batch_size=self.batch_size * 2, num_workers=4
        )
        
        # æœ€ç»ˆä¿®æ­£: æ ¹æ®è°ƒè¯•ç»“æœï¼Œä½¿ç”¨ä¸¤ä¸ªå˜é‡ç›´æ¥è§£åŒ…
        val_raw_output, val_index_df = best_tft.predict(val_dataloader, mode="raw", return_index=True)

        # æœ€ç»ˆä¿®æ­£: åŸå§‹è¾“å‡ºä¸ºç±»å­—å…¸å¯¹è±¡ï¼Œç›´æ¥é€šè¿‡é”® "prediction" å–æ ¸å¿ƒé¢„æµ‹å¼ é‡
        val_param_tensor = val_raw_output["prediction"]

        if isinstance(val_param_tensor, np.ndarray):
            val_param_tensor = torch.from_numpy(val_param_tensor)
        if isinstance(val_param_tensor, torch.Tensor):
            val_param_tensor = val_param_tensor.cpu()

        # åŸºäºæ³Šæ¾åˆ†å¸ƒå‚æ•°è¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°è®¡æ•°å‹æ ·æœ¬ï¼ˆç›®æ ‡é‡‡ç”¨ identityï¼Œæ— éœ€é€†å˜æ¢ï¼‰
        # 1. é¦–å…ˆï¼Œåº”ç”¨æ¿€æ´»å‡½æ•°å°†åŸå§‹è¾“å‡º(logits)è½¬æ¢ä¸ºéè´Ÿçš„ç‡å‚æ•°(rate)
        #    è¿™ä¸è®­ç»ƒæ—¶PoissonLosså†…éƒ¨çš„æ“ä½œä¸€è‡´
        positive_rate_tensor = torch.exp(val_param_tensor)
        
        # 2. ç„¶åï¼Œä½¿ç”¨è½¬æ¢åçš„æ­£ç‡å‚æ•°è¿›è¡Œæ³Šæ¾é‡‡æ ·
        val_samples_tensor = torch.poisson(positive_rate_tensor)
        val_samples_np = val_samples_tensor.detach().cpu().numpy()

        # éè´Ÿè£å‰ª + 0.1 é˜ˆå€¼ç½®é›¶ï¼ˆä¿è¯ä¸ä¸šåŠ¡è§„åˆ™ä¸€è‡´ï¼‰
        # æ³¨ï¼šæ³Šæ¾é‡‡æ ·ç†è®ºä¸Šè¾“å‡ºéè´Ÿæ•´æ•°ï¼Œä½†ä¿ç•™é˜ˆå€¼ä»¥é˜²æœªæ¥åˆ‡æ¢ä¸ºè¿ç»­é¢„æµ‹
        val_predictions_non_negative = np.maximum(0, val_samples_np)
        val_final_predictions = np.where(val_predictions_non_negative < 0.1, 0, val_predictions_non_negative)
        
        # å¤„ç†éªŒè¯é›†é¢„æµ‹ç»“æœ
        val_prediction_df = pd.DataFrame()
        for i in range(self.prediction_length):
            temp_df = val_index_df.copy()
            temp_df['prediction'] = val_final_predictions[:, i]
            temp_df['time_idx'] = temp_df['time_idx'] + i
            val_prediction_df = pd.concat([val_prediction_df, temp_df])
        
        # åˆå¹¶éªŒè¯é›†çœŸå®å€¼ä¸é¢„æµ‹å€¼
        val_actual_df = df[required_columns + ['sdeptname']].copy()
        val_evaluation_df = pd.merge(
            val_actual_df, 
            val_prediction_df, 
            on=['time_idx', 'store_id', 'item_id'], 
            how='inner'
        )
        
        if len(val_evaluation_df) == 0:
            raise ValueError("éªŒè¯é›†é¢„æµ‹ç»“æœä¸çœŸå®å€¼æ— æ³•åŒ¹é…")
        
        print(f"   éªŒè¯é›†åŒ¹é…åˆ° {len(val_evaluation_df)} æ¡è¯„ä¼°è®°å½•")
        
        # è¡¥å……dtdateå­—æ®µç”¨äºå¯è§†åŒ–
        val_evaluation_df = val_evaluation_df.merge(
            df[['store_id', 'item_id', 'time_idx', 'dtdate']],
            on=['store_id', 'item_id', 'time_idx'],
            how='left'
        )
        
        # æŒ‰å¤©è¯„ä¼°éªŒè¯é›†æŒ‡æ ‡
        val_daily_actual = val_evaluation_df['sales'].values
        val_daily_pred = val_evaluation_df['prediction'].values
        val_daily_non_zero_mask = val_daily_actual > 0
        val_daily_non_zero_wape = calculate_wape(val_daily_actual[val_daily_non_zero_mask], val_daily_pred[val_daily_non_zero_mask]) if val_daily_non_zero_mask.sum() > 0 else 0
        
        # æŒ‰æœˆè¯„ä¼°éªŒè¯é›†æŒ‡æ ‡
        # æŒ‰å•†å“-åº—é“º-æœˆä»½åˆ†ç»„æ±‚å’Œ
        val_evaluation_df['year_month'] = val_evaluation_df['dtdate'].dt.to_period('M')
        val_monthly = val_evaluation_df.groupby(['store_id', 'item_id', 'year_month']).agg({
            'sales': 'sum',
            'prediction': 'sum'
        }).reset_index()
        
        val_monthly_actual = val_monthly['sales'].values
        val_monthly_pred = val_monthly['prediction'].values
        val_monthly_non_zero_mask = val_monthly_actual > 0
        val_monthly_non_zero_wape = calculate_wape(val_monthly_actual[val_monthly_non_zero_mask], val_monthly_pred[val_monthly_non_zero_mask]) if val_monthly_non_zero_mask.sum() > 0 else 0
        
        results['validation'] = {
            # æŒ‰å¤©æŒ‡æ ‡
            'daily_wape': calculate_wape(val_daily_actual, val_daily_pred),
            'daily_mae': calculate_mae(val_daily_actual, val_daily_pred),
            'daily_rmse': calculate_rmse(val_daily_actual, val_daily_pred),
            'daily_total_actual': np.sum(val_daily_actual),
            'daily_total_predicted': np.sum(val_daily_pred),
            'daily_prediction_bias': (np.sum(val_daily_pred) - np.sum(val_daily_actual)) / np.sum(val_daily_actual) if np.sum(val_daily_actual) > 0 else 0,
            'daily_non_zero_wape': val_daily_non_zero_wape,
            'daily_non_zero_count': val_daily_non_zero_mask.sum(),
            'daily_record_count': len(val_daily_actual),
            
            # æŒ‰æœˆæŒ‡æ ‡
            'monthly_wape': calculate_wape(val_monthly_actual, val_monthly_pred),
            'monthly_mae': calculate_mae(val_monthly_actual, val_monthly_pred),
            'monthly_rmse': calculate_rmse(val_monthly_actual, val_monthly_pred),
            'monthly_total_actual': np.sum(val_monthly_actual),
            'monthly_total_predicted': np.sum(val_monthly_pred),
            'monthly_prediction_bias': (np.sum(val_monthly_pred) - np.sum(val_monthly_actual)) / np.sum(val_monthly_actual) if np.sum(val_monthly_actual) > 0 else 0,
            'monthly_non_zero_wape': val_monthly_non_zero_wape,
            'monthly_non_zero_count': val_monthly_non_zero_mask.sum(),
            'monthly_record_count': len(val_monthly_actual),
            
            # è¯¦ç»†é¢„æµ‹æ•°æ®
            'detailed_predictions': val_evaluation_df  # æ·»åŠ è¯¦ç»†çš„é¢„æµ‹æ•°æ®ï¼ˆåŒ…å«dtdateï¼‰
        }
        
        print(f"âœ… éªŒè¯é›†è¯„ä¼°å®Œæˆ - æŒ‰å¤©WAPE: {results['validation']['daily_wape']:.4f}, æŒ‰æœˆWAPE: {results['validation']['monthly_wape']:.4f}")

        # 5. é¢„æµ‹æµ‹è¯•é›† (6æœˆä»½)
        print("   é¢„æµ‹æµ‹è¯•é›† (6æœˆä»½)...")
        
        # æ‰¾åˆ°æµ‹è¯•é›†é¢„æµ‹æ‰€éœ€å†å²æ•°æ®çš„æˆªæ­¢ç‚¹
        test_encoder_cutoff_date = pd.to_datetime("2025-05-31")
        test_encoder_cutoff_idx = df[df['dtdate'] == test_encoder_cutoff_date]['time_idx'].iloc[0]
        
        # ä»åŸå§‹è®­ç»ƒé›†ç»“æ„å‡ºå‘ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼ŒåŒ…å«5æœˆä»½æ•°æ®
        test_dataset = TimeSeriesDataSet.from_dataset(
            self.training_dataset,
            df,  # ä¼ å…¥åŒ…å«æ‰€æœ‰ï¼ˆç›´åˆ°6æœˆ30æ—¥ï¼‰æ•°æ®çš„å®Œæ•´DataFrame
            predict=True,
            stop_randomization=True,
            min_prediction_idx=test_encoder_cutoff_idx + 1,
            min_encoder_length=0,  # æ˜¾å¼è®¾ç½®min_encoder_length=0
            add_relative_time_idx=True
        )
        
        # åˆ›å»ºæµ‹è¯•é›†çš„dataloader
        test_dataloader = test_dataset.to_dataloader(
            train=False, batch_size=self.batch_size * 2, num_workers=4
        )
        
        # è¿›è¡Œæµ‹è¯•é›†é¢„æµ‹
        # ä½¿ç”¨åŸå§‹è¾“å‡ºä»¥è·å–åˆ†å¸ƒå‚æ•°ï¼ˆæœ€ç»ˆä¿®æ­£ï¼šä¸¤ä¸ªè¿”å›å€¼ï¼‰
        test_raw_output, test_index_df = best_tft.predict(test_dataloader, mode="raw", return_index=True)

        # æœ€ç»ˆä¿®æ­£: åŸå§‹è¾“å‡ºä¸ºç±»å­—å…¸å¯¹è±¡ï¼Œç›´æ¥é€šè¿‡é”® "prediction" å–æ ¸å¿ƒé¢„æµ‹å¼ é‡
        test_param_tensor = test_raw_output["prediction"]

        if isinstance(test_param_tensor, np.ndarray):
            test_param_tensor = torch.from_numpy(test_param_tensor)
        if isinstance(test_param_tensor, torch.Tensor):
            test_param_tensor = test_param_tensor.cpu()

        # åŸºäºæ³Šæ¾åˆ†å¸ƒå‚æ•°è¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ°è®¡æ•°å‹æ ·æœ¬ï¼ˆç›®æ ‡é‡‡ç”¨ identityï¼Œæ— éœ€é€†å˜æ¢ï¼‰
        # 1. åŒæ ·åœ°ï¼Œå¯¹æµ‹è¯•é›†çš„åŸå§‹è¾“å‡ºåº”ç”¨æ¿€æ´»å‡½æ•°
        positive_rate_tensor_test = torch.exp(test_param_tensor)
        
        # 2. ä½¿ç”¨è½¬æ¢åçš„æ­£ç‡å‚æ•°è¿›è¡Œé‡‡æ ·
        test_samples_tensor = torch.poisson(positive_rate_tensor_test)
        test_samples_np = test_samples_tensor.detach().cpu().numpy()

        # éè´Ÿè£å‰ª + 0.1 é˜ˆå€¼ç½®é›¶
        # æ³¨ï¼šæ³Šæ¾é‡‡æ ·ç†è®ºä¸Šè¾“å‡ºéè´Ÿæ•´æ•°ï¼Œä½†ä¿ç•™é˜ˆå€¼ä»¥é˜²æœªæ¥åˆ‡æ¢ä¸ºè¿ç»­é¢„æµ‹
        test_predictions_non_negative = np.maximum(0, test_samples_np)
        test_final_predictions = np.where(test_predictions_non_negative < 0.1, 0, test_predictions_non_negative)
        
        # å¤„ç†æµ‹è¯•é›†é¢„æµ‹ç»“æœ
        test_prediction_df = pd.DataFrame()
        for i in range(self.prediction_length):
            temp_df = test_index_df.copy()
            temp_df['prediction'] = test_final_predictions[:, i]
            temp_df['time_idx'] = temp_df['time_idx'] + i
            test_prediction_df = pd.concat([test_prediction_df, temp_df])
        
        # åˆå¹¶æµ‹è¯•é›†çœŸå®å€¼ä¸é¢„æµ‹å€¼
        test_actual_df = df[required_columns + ['sdeptname']].copy()
        test_evaluation_df = pd.merge(
            test_actual_df, 
            test_prediction_df, 
            on=['time_idx', 'store_id', 'item_id'], 
            how='inner'
        )
        
        if len(test_evaluation_df) == 0:
            raise ValueError("æµ‹è¯•é›†é¢„æµ‹ç»“æœä¸çœŸå®å€¼æ— æ³•åŒ¹é…")
        
        print(f"   æµ‹è¯•é›†åŒ¹é…åˆ° {len(test_evaluation_df)} æ¡è¯„ä¼°è®°å½•")
        
        # è¡¥å……dtdateå­—æ®µç”¨äºå¯è§†åŒ–
        test_evaluation_df = test_evaluation_df.merge(
            df[['store_id', 'item_id', 'time_idx', 'dtdate']],
            on=['store_id', 'item_id', 'time_idx'],
            how='left'
        )
        
        # æŒ‰å¤©è¯„ä¼°æµ‹è¯•é›†æŒ‡æ ‡
        test_daily_actual = test_evaluation_df['sales'].values
        test_daily_pred = test_evaluation_df['prediction'].values
        test_daily_non_zero_mask = test_daily_actual > 0
        test_daily_non_zero_wape = calculate_wape(test_daily_actual[test_daily_non_zero_mask], test_daily_pred[test_daily_non_zero_mask]) if test_daily_non_zero_mask.sum() > 0 else 0
        
        # æŒ‰æœˆè¯„ä¼°æµ‹è¯•é›†æŒ‡æ ‡
        # æŒ‰å•†å“-åº—é“º-æœˆä»½åˆ†ç»„æ±‚å’Œ
        test_evaluation_df['year_month'] = test_evaluation_df['dtdate'].dt.to_period('M')
        test_monthly = test_evaluation_df.groupby(['store_id', 'item_id', 'year_month']).agg({
            'sales': 'sum',
            'prediction': 'sum'
        }).reset_index()
        
        test_monthly_actual = test_monthly['sales'].values
        test_monthly_pred = test_monthly['prediction'].values
        test_monthly_non_zero_mask = test_monthly_actual > 0
        test_monthly_non_zero_wape = calculate_wape(test_monthly_actual[test_monthly_non_zero_mask], test_monthly_pred[test_monthly_non_zero_mask]) if test_monthly_non_zero_mask.sum() > 0 else 0
        
        results['test'] = {
            # æŒ‰å¤©æŒ‡æ ‡
            'daily_wape': calculate_wape(test_daily_actual, test_daily_pred),
            'daily_mae': calculate_mae(test_daily_actual, test_daily_pred),
            'daily_rmse': calculate_rmse(test_daily_actual, test_daily_pred),
            'daily_total_actual': np.sum(test_daily_actual),
            'daily_total_predicted': np.sum(test_daily_pred),
            'daily_prediction_bias': (np.sum(test_daily_pred) - np.sum(test_daily_actual)) / np.sum(test_daily_actual) if np.sum(test_daily_actual) > 0 else 0,
            'daily_non_zero_wape': test_daily_non_zero_wape,
            'daily_non_zero_count': test_daily_non_zero_mask.sum(),
            'daily_record_count': len(test_daily_actual),
            
            # æŒ‰æœˆæŒ‡æ ‡
            'monthly_wape': calculate_wape(test_monthly_actual, test_monthly_pred),
            'monthly_mae': calculate_mae(test_monthly_actual, test_monthly_pred),
            'monthly_rmse': calculate_rmse(test_monthly_actual, test_monthly_pred),
            'monthly_total_actual': np.sum(test_monthly_actual),
            'monthly_total_predicted': np.sum(test_monthly_pred),
            'monthly_prediction_bias': (np.sum(test_monthly_pred) - np.sum(test_monthly_actual)) / np.sum(test_monthly_actual) if np.sum(test_monthly_actual) > 0 else 0,
            'monthly_non_zero_wape': test_monthly_non_zero_wape,
            'monthly_non_zero_count': test_monthly_non_zero_mask.sum(),
            'monthly_record_count': len(test_monthly_actual),
            
            # è¯¦ç»†é¢„æµ‹æ•°æ®
            'detailed_predictions': test_evaluation_df  # æ·»åŠ è¯¦ç»†çš„é¢„æµ‹æ•°æ®ï¼ˆåŒ…å«dtdateï¼‰
        }
        
        print(f"âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆ - æŒ‰å¤©WAPE: {results['test']['daily_wape']:.4f}, æŒ‰æœˆWAPE: {results['test']['monthly_wape']:.4f}")
        
        # æŒä¹…åŒ–ä¿å­˜TFTæµ‹è¯•é›†è¯¦ç»†é¢„æµ‹ç»“æœ
        cols_to_save = ['store_id', 'item_id', 'sales', 'dtdate', 'time_idx', 'prediction', 'sdeptname']
        test_evaluation_df[cols_to_save].to_csv('tft_test_predictions.csv', index=False)
        print("âœ… å·²ä¿å­˜TFTæµ‹è¯•é›†è¯¦ç»†é¢„æµ‹ç»“æœåˆ° tft_test_predictions.csv")

        return results
    
    def fit(self, file_path):
        """
        å®Œæ•´çš„TFTæ¨¡å‹è®­ç»ƒæµç¨‹
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        print("ğŸš€ å¼€å§‹TFTæ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹")
        
        # æ­¥éª¤1: æ•°æ®åŠ è½½ä¸é¢„å¤„ç†
        df = self.load_and_preprocess_data(file_path)
        
        # æ­¥éª¤2: ç‰¹å¾åˆ†ç»„å®šä¹‰
        feature_groups = self.define_feature_groups()
        
        # æ­¥éª¤3: åˆ›å»ºTimeSeriesDataSet
        self.create_timeseries_dataset(df, feature_groups)
        
        # æ­¥éª¤4: åˆ›å»ºæ¨¡å‹
        self.create_model()
        
        # æ­¥éª¤5: è®­ç»ƒæ¨¡å‹
        self.train_model()
        
        print("ğŸ‰ TFTæ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆ")
        return self.model

if __name__ == "__main__":
    # æµ‹è¯•TFTæ¨¡å‹
    from data_analysis import load_and_analyze_data
    
    # åŠ è½½æ•°æ®
    df, train_mask, val_mask, test_mask = load_and_analyze_data()
    
    # è®­ç»ƒTFTæ¨¡å‹
    tft_model = TFTModel(
        prediction_length=30,
        encoder_length=90,
        learning_rate=0.0002,
        hidden_size=64,
        attention_head_size=8,
        dropout=0.2,
        hidden_continuous_size=32,
        batch_size=1024,
        max_epochs=30,  
        patience=5
    )
    
    # è®­ç»ƒæ¨¡å‹
    model = tft_model.fit('model_data_top10percent.csv')
    
    # è·å–é¢„å¤„ç†åçš„æ•°æ®ç”¨äºè¯„ä¼°
    df = tft_model.load_and_preprocess_data('model_data_top10percent.csv')
    
    # è¯„ä¼°æ¨¡å‹
    results = tft_model.predict_and_evaluate(df)
    
    print("\nğŸ“‹ TFTæ¨¡å‹è¯„ä¼°ç»“æœ:")
    for dataset, metrics in results.items():
        print(f"\n{dataset.upper()} é›†:")
        print("æŒ‰å¤©æŒ‡æ ‡:")
        print(f"  daily_wape: {metrics['daily_wape']:.4f}")
        print(f"  daily_mae: {metrics['daily_mae']:.4f}")
        print(f"  daily_rmse: {metrics['daily_rmse']:.4f}")
        print(f"  daily_prediction_bias: {metrics['daily_prediction_bias']:.4f}")
        print(f"  daily_non_zero_wape: {metrics['daily_non_zero_wape']:.4f}")
        print("æŒ‰æœˆæŒ‡æ ‡:")
        print(f"  monthly_wape: {metrics['monthly_wape']:.4f}")
        print(f"  monthly_mae: {metrics['monthly_mae']:.4f}")
        print(f"  monthly_rmse: {metrics['monthly_rmse']:.4f}")
        print(f"  monthly_prediction_bias: {metrics['monthly_prediction_bias']:.4f}")
        print(f"  monthly_non_zero_wape: {metrics['monthly_non_zero_wape']:.4f}") 